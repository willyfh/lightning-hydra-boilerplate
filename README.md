<div align="center">
<h1> âš¡ Lightning-Hydra-Boilerplate </h1>

[![python](https://img.shields.io/badge/python-3.12-blue)]() [![Run Tests](https://github.com/willyfh/lightning-hydra-boilerplate/actions/workflows/pytest.yaml/badge.svg)](https://github.com/willyfh/lightning-hydra-boilerplate/actions/workflows/pytest.yaml)

</div>

A project boilerplate for deep learning experiments using PyTorch Lightning and Hydra, designed for rapid prototyping, clean configuration management, and scalable research workflows.

âš ï¸ _This project is in its early stages and still under active development. Expect breaking changes and incomplete features._

## ğŸ“ Project Structure

```plaintext
lightning-hydra-boilerplate/
â”‚â”€â”€ configs/                  # YAML configurations for Hydra
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ example_data.yaml
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ example_model.yaml
â”‚   â”œâ”€â”€ trainer/
â”‚   â”‚   â”œâ”€â”€ default.yaml
â”‚   â”œâ”€â”€ train.yaml
â”‚   â”œâ”€â”€ eval.yaml
â”‚   â”œâ”€â”€ predict.yaml
â”‚
â”‚â”€â”€ src/                       # Core codebase
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ example_data/
â”‚   â”‚   â”‚   â”œâ”€â”€ lightning_datamodule.py
â”‚   â”‚   â”‚   â”œâ”€â”€ torch_dataset.py
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ example_model/
â”‚   â”‚   â”‚   â”œâ”€â”€ lightning_module.py
â”‚   â”‚   â”‚   â”œâ”€â”€ torch_model.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ train.py               # Training entrypoint
â”‚   â”œâ”€â”€ eval.py                # Evaluation entrypoint
â”‚   â”œâ”€â”€ predict.py             # Inference entrypoint (for making predictions)
â”‚
â”‚â”€â”€ tests/                     # Unit tests
â”‚
â”‚â”€â”€ .gitignore
â”‚â”€â”€ LICENSE
â”‚â”€â”€ poetry.lock
â”‚â”€â”€ pyproject.toml
â”‚â”€â”€ README.md
```

## ğŸš€ Getting Started

### **1ï¸âƒ£ Install Dependencies**

This project uses **Poetry** for dependency management.

```bash
pip install poetry  # If you haven't installed Poetry yet
poetry install
```

### **2ï¸âƒ£ Train a Model**

Run the training script with the default configurations:

```bash
 poetry run python src/train.py
```

This will:

- Train the model on the training set
- Validate during training using the validation set
- Test the final model on the test set (unless `skip_test=true` is set)

### **3ï¸âƒ£ Evaluate a Model**

If you need to run evaluation independently on a specific checkpoint and dataset split (val or test), use the evaluation script:

```bash
poetry run python src/eval.py data_split=test ckpt_path=/path/to/checkpoint.ckpt
```

### **4ï¸âƒ£ Predict Outputs**

To generate predictions from a trained model (e.g., for submission or analysis), run the predict.py script with the path to your checkpoint:

```bash
poetry run python src/predict.py ckpt_path=/path/to/checkpoint.ckpt
```

This will:

- Run inference on the prediction set (defaults to `test` set)
- Save the results to a file (e.g., predictions.csv or predictions.json depending on `save_format`) under the Hydra run directory

### **5ï¸âƒ£ Experiment Configuration**

All experiment settings are managed with Hydra.

You can modify the configs under configs/ or override them via CLI. For example:

```bash
 poetry run python src/train.py trainer.max_epochs=10
```

More info: https://hydra.cc/docs/intro/

### **6ï¸âƒ£ Outputs**

All run outputs are saved under the `outputs/` directory:

```plaintext
outputs/
â”œâ”€â”€ train/    â† training logs, checkpoints, config snapshots
â”œâ”€â”€ eval/     â† evaluation logs and results
â”œâ”€â”€ predict/  â† prediction files (e.g. .csv, .json)
```

Each run is timestamped for easy tracking and reproducibility.

## âœ… Completed Tasks

- [x] Initial project setup with PyTorch Lightning and Hydra
- [x] Basic training workflow with example model and data
- [x] TensorBoard logging support
- [x] Poetry setup
- [x] Configurable Callbacks (EarlyStopping, ModelCheckpoint, etc.)
- [x] Run test after training
- [x] Setup pre-commit setup
- [x] Setup tests
- [x] Setup dependabot
- [x] Add logger
- [x] Organize `logs/`, `checkpoints/` (Lightning), and `outputs/` (Hydra) properly
- [x] Evaluation script (`eval.py`)
- [x] Inference script (`predict.py`)
- [x] Make optimizer configurable

## ğŸ“ TODO List

âš ï¸ _Feel free to fork the repo, create a PR, or open an issue if you spot anything or have ideas. Iâ€™d love to hear your feedback and make this more useful for everyone!_

- [ ] Hyperparameter tuning with Optuna
- [ ] Check Multi-GPU
- [ ] Add more Lightning Trainer features (resume, callbacks, etc.)
- [ ] MLflow and/or Wandb
- [ ] Docker support for easy deployment
- [ ] Make metrics configurable
- [ ] Add task-specific examples and configs (e.g., object detection, text classification, etc.)
- [ ] Add experiment configs for reusable training/eval setups (e.g., `configs/experiments/exp1.yaml`)

## ğŸ“œ License

This project is licensed under the MIT License.
